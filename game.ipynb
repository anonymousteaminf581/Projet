{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Installation of the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the environment, you have to install the package gym-foo with **pip install -e gym-foo** were the directory is located (at least on linux)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy\n",
    "env = gym.make('gym_foo:foo-v0', cr = 7, pr = 10, np = 2, ng = 10, alphas = [.5,.5]) # you can pass the arguments of the __init__ method here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Basic example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of the 10 turns.\n",
      " iter | p1 | p2 \n",
      "     1| 0  | 1  \n",
      "     2| 1  | 1  \n",
      "     3| 1  | 0  \n",
      "     4| 0  | 0  \n",
      "     5| 1  | 0  \n",
      "     6| 1  | 0  \n",
      "     7| 0  | 1  \n",
      "     8| 1  | 1  \n",
      "     9| 0  | 1  \n",
      "    10| 1  | 0  \n",
      "scores|123 |113 \n",
      " util |123 |113 \n",
      "\n",
      "p1 has won !\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(1):\n",
    "    observation = env.reset()\n",
    "    for t in range(10):\n",
    "#         env.render()\n",
    "#         print(observation)\n",
    "        action = numpy.random.randint(0,2,env.num_of_players)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "#         print(reward)\n",
    "        if done:\n",
    "            env.close()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Test with Q-Learning and tables\n",
    "We initialize with the reward of a 1-turn game according to the number of cooperating agent and the action the agent has chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement a function to enhance the lisibility of the result tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpretation(table):\n",
    "    num_of_players = table.shape[1]\n",
    "    print(\"num of cooperating players | Q-value if cooperating | Q-value if not\")\n",
    "    for i,line in enumerate(table):\n",
    "        print(\"{0:^26} | {1:^22} | {2:^14}\".format(i,*line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 $\\epsilon$-greedy selection, $\\epsilon$ decreasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = [numpy.zeros((env.num_of_players,2)) for player in env.player]\n",
    "alpha = .9\n",
    "gamma = .85\n",
    "epsilon = 0.5\n",
    "\n",
    "for i_episode in range(100000):\n",
    "    s_t1 = env.reset()\n",
    "    \n",
    "    for t in range(10):\n",
    "#         env.render()\n",
    "        if t == 0:\n",
    "            s_t1 = [0 for player in env.player] # We start with the idea that everyone is cooperating. That is a choice.\n",
    "            \n",
    "        a_t1 = numpy.zeros(env.num_of_players, dtype = \"int\")\n",
    "        for i,player in enumerate(env.player):\n",
    "            # The only relevant information is the number of agents who chose to cooperate.\n",
    "            # We simplify the state with the number of cooperating agents.\n",
    "            # We will update a table in order to find the best action according to the number of cooperating players\n",
    "            \n",
    "            coop = env.num_of_players - 1 - (sum(s_t1) - s_t1[i])\n",
    "            best_action = numpy.argmax(tables[i][coop])\n",
    "            if 1-epsilon/(i_episode + 1) > numpy.random.uniform():  \n",
    "                a_t1[i] = best_action\n",
    "            else:\n",
    "                a_t1[i] = 1 - best_action\n",
    "\n",
    "        s_t2, r_t1, done, info = env.step(a_t1)\n",
    "        for i,player in enumerate(env.player):\n",
    "            tab = tables[i]\n",
    "            coop_t1, coop_t2 = env.num_of_players - 1 - (sum(s_t1) - s_t1[i]), env.num_of_players - 1 -(sum(s_t2) - s_t2[i])\n",
    "\n",
    "            tab[coop_t1][a_t1[i]] += alpha * (r_t1[i] + gamma * max(tab[coop_t2]) - tab[coop_t1][a_t1[i]])\n",
    "        \n",
    "        s_t1 = s_t2\n",
    "            \n",
    "        if done:\n",
    "#             env.close()\n",
    "#             print(tables)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of cooperating players | Q-value if cooperating | Q-value if not\n",
      "            0              |   11.431453563011264   | 59.977234732446306\n",
      "            1              |   3.024878553867845    | 108.39577659117276\n"
     ]
    }
   ],
   "source": [
    "for player, table in zip(env.player, tables):\n",
    "    print(player.name)\n",
    "    interpretation(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Softmax selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = [numpy.zeros((env.num_of_players,2)) for player in env.player]\n",
    "for table in tables:\n",
    "    for i in range(table.shape[1]):\n",
    "        table[i][0], table[i][1] = env.collective_reward*(i+1),env.collective_reward*i+env.personal_reward \n",
    "\n",
    "alpha = .9\n",
    "gamma = .85\n",
    "tau = 1\n",
    "\n",
    "for i_episode in range(100000):\n",
    "    s_t1 = env.reset()\n",
    "    \n",
    "    for t in range(10):\n",
    "#         env.render()\n",
    "        if t == 0:\n",
    "            s_t1 = [0 for player in env.player] # We start with the idea that everyone is cooperating. That is a choice.\n",
    "            \n",
    "        a_t1 = numpy.zeros(env.num_of_players, dtype = \"int\")\n",
    "        for i,player in enumerate(env.player):\n",
    "            # The only relevant information is the number of agents who chose to cooperate.\n",
    "            # We simplify the state with the number of cooperating agents.\n",
    "            # We will update a table in order to find the best action according to the number of cooperating players\n",
    "            \n",
    "            coop = env.num_of_players - 1 - (sum(s_t1) - s_t1[i])\n",
    "            p0,p1 = numpy.exp(tables[i][coop,0]),numpy.exp(tables[i][coop,1])\n",
    "            if p0/(p0+p1) > numpy.random.uniform():  \n",
    "                a_t1[i] = 0\n",
    "            else:\n",
    "                a_t1[i] = 1\n",
    "\n",
    "        s_t2, r_t1, done, info = env.step(a_t1)\n",
    "        for i,player in enumerate(env.player):\n",
    "            tab = tables[i]\n",
    "            coop_t1, coop_t2 = env.num_of_players - 1 - (sum(s_t1) - s_t1[i]), env.num_of_players - 1 -(sum(s_t2) - s_t2[i])\n",
    "\n",
    "            tab[coop_t1][a_t1[i]] += alpha * (r_t1[i] + gamma * max(tab[coop_t2]) - tab[coop_t1][a_t1[i]])\n",
    "        \n",
    "        s_t1 = s_t2\n",
    "            \n",
    "        if done:\n",
    "#             env.close()\n",
    "#             print(tables)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1\n",
      "num of cooperating players | Q-value if cooperating | Q-value if not\n",
      "            0              |   5.449937367191885    | 62.25362965196577\n",
      "            1              |   5.203359994263341    | 39.58310395204195\n",
      "p2\n",
      "num of cooperating players | Q-value if cooperating | Q-value if not\n",
      "            0              |   7.819217800007606    | 4.628774978267523\n",
      "            1              |   16.846841713121172   | 3.072675031502081\n"
     ]
    }
   ],
   "source": [
    "for player, table in zip(env.player, tables):\n",
    "    print(player.name)\n",
    "    interpretation(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
